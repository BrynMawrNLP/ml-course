---
layout: default
title: Logistic Regression
type: Homework
number: 05
active_tab: homework
release_date: 2024-10-08
due_date: 2024-10-22 23:59:00EDT

---

<!-- Check whether the assignment is ready to release -->
{% capture today %}{{'now' | date: '%s'}}{% endcapture %}
{% capture release_date %}{{page.release_date | date: '%s'}}{% endcapture %}
{% if release_date > today %} 
<div class="alert alert-danger">
Warning: this assignment is out of date.  It may still need to be updated for this year's class.  Check with your instructor before you start working on this assignment.
</div>
{% endif %}
<!-- End of check whether the assignment is up to date -->


<!-- Check whether the assignment is up to date -->
{% capture this_year %}{{'now' | date: '%Y'}}{% endcapture %}
{% capture due_year %}{{page.due_date | date: '%Y'}}{% endcapture %}
{% if this_year != due_year %} 
<div class="alert alert-danger">
Warning: this assignment is out of date.  It may still need to be updated for this year's class.  Check with your instructor before you start working on this assignment.
</div>
{% endif %}
<!-- End of check whether the assignment is up to date -->



{% if page.materials %}
<div class="alert alert-info">
You can download the materials for this assignment here:
<ul>
{% for item in page.materials %}
<li><a href="{{item.url}}">{{ item.name }}</a></li>
{% endfor %}
</ul>


<i>Remember to make a copy of the notebook into your own Drive by choosing “Save a Copy in Drive” from Colab’s “File” menu.</i>

</div>
{% endif %}





{{page.type}} {{page.number}}: {{page.title}}
=============================================================

_Due: {{page.due_date}}_

## Overview

The goals of this week’s assignment are:

- Understand and implement Logistic Regression
- Apply Logistic Regression to a real-world dataset


In this homework we will be analyzing a [phoneme](https://en.wikipedia.org/wiki/Phoneme) dataset:

- The task of phoneme classification is to predict the label of a phoneme articulation given its corresponding voice signal. In this problem, we have voice signals from two different phonemes: “aa” (as in “bot”) and “ae” (as in “bat”). They are from an acoustic-phonetic corpus called [TIMIT](https://catalog.ldc.upenn.edu/LDC93S1). Phoneme classification is an important part of a speech-to-text system.


This assignment is to be done individually. Discussing high-level ideas with others in the class is encouraged, but you should not be sharing code in any way.

### Provided Code

You should have the following files:

- `input` - directory containing train/test data sets. You may add additional data sets here.
- `run_LR.py` - your main program executable for Logistic Regression.
- `LogisticRegression.py` - file for the `LogisticRegression` class (and/or functions).
- `README.md` - for analysis questions and feedback.

### Usage and I/O

#### Usage

Your programs should take in the same command-line arguments as Homework 4 (feel free to reuse the argument parsing code), plus a parameter for the learning rate alpha. For example, to run Logistic Regression on the phoneme example:

```unix
python3 run_LR.py -r input/phoneme_train.csv -e input/phoneme_test.csv -a 0.02
```

#### Program Inputs
To simplify preprocessing, you may assume the following:

- The datasets will have continuous features, and binary labels (0, 1). The train and test datasets will be in CSV form (comma separated values), with the label as the last column.
- I would recommend creating a function to parse the CSV file format. Use the method `split(',')` to split the line (type string) into a list of strings, which can be parsed further.
- Make sure the user enters a positive `alpha` value (it should be type float)

#### Program Outputs
- Your program should print a confusion matrix and accuracy of the result (see examples below).

## Logistic Regression
You will implement the logistic regression task discussed in class for binary classification.

### Model

For now you should assume that the features will be continuous and the response will be a discrete, binary output. In the case of binary labels, the probability of a positive prediction is:

$$
h_{w}(x) =  p(y=1 | x) = \dfrac{1}{1 + e^{-w \dot x}}
$$

To model the intercept term, we will include a bias term. In practice, this is equivalent to adding a feature that is always "on". For each instance in training and testing, append a <b>1</b> to the feature vector. Ensure that your logistic regression model has a weight for this feature that it can now learn in the same way it learns all other weights.

### Training

To learn the weights, you will apply stochastic gradient descent as discussed in class until the cost function does not change in value (very much) for a given iteration. As a reminder, our cost function is the negative log of the likelihood function, which is:

$$
L(w) = \prod_{i=1}^{n}h_{w}(x_{i})^{y_{i}}(1 - h_{w}(x_{i}))^{1-y_{i}}
$$

Our goal is to minimize the cost using SGD. We will use the same idea as in Homework 3 for linear regression. Pseudocode:

```python
initialize weights to 0s
while not converged:
    shuffle the training examples
    for each training example xi:
        calculate derivative of cost with respect to xi
        weights = weights - alpha*derivative
    compute and store current cost
```

The SGD updates for $$w$$ term are:

$$
w \leftarrow w - \alpha (h_{w}(x_{i}) - y_{i})x_{i} 
$$

The hyper-parameter <i>alpha</i> (learning rate) should be sent as a parameter to your SGD and used in training. A few notes for the above:

- In Homework 3 we did not actually shuffle the data points to make gradient descent truly "stochastic", but for this homework you should (also make sure $$y$$ is shuffled along with $$X$$!). You may use:

```python
numpy.random.shuffle(A)
```

which by default shuffles along the first axis.

- The stopping criteria should be a) a maximum of some number of iterations (your choice) OR b) the cost has changed by less some number between two iterations (your choice).

- Many of the operations above are on vectors. I recommend using `numpy` features such as dot product to make the code simple.

- Try to choose hyper-parameters that maximize the testing accuracy.


### Prediction

For binary prediction, apply the equation:

$$
h_{w}(x) = p(y=1 | x) = \dfrac{1}{1 + e^{-w \dot x}}
$$

and output the more likely binary outcome. You may use the Python `math.exp` function to make the calculation in the denominator. Alternatively, you may use the weights directly as a decision boundary as discussed in class.

The choice of hyper-parameters will affect the results, but you should be able to obtain at least 90% accuracy on the testing data. Make sure to print a confusion matrix.

### Example Output

Here is an example run for testing purposes. You should be able to get a better result than this, but this will check if your algorithm is working. The details are:

- No shuffling of the training data points (which you should do for the final result!)
- `alpha` = 0.02
- stopping criteria: looking for changes in the cost `J(w)`
- `epsilon` = 1e-4 (see if cost has changed by this amount)

```unix
$ python3 run_LR.py -r input/phoneme_train.csv -e input/phoneme_test.csv -a 0.02
Accuracy: 0.875000 (70 out of 80 correct)

   prediction
      0  1
    ------
  0| 36  4
  1|  6 34
```

## Analysis and Comparison

(See README.md to answer these questions)

1. So far, we have run logistic regression and Naive Bayes on different types of datasets. Why is that? Discuss the types of features and labels that we worked with in each case.

1. Explain how you could transform the CSV dataset with continuous features to work for Naive Bayes. You could use something similar to what we did for decision trees, but try to think about alternatives.

1. Then explain how you could transform the ARFF datasets with discrete features to work with logistic regression. What issues need to be overcome to apply logistic regression to the zoo dataset?

## Extension (optional)
Please include a discussion of your extension in your README. If this assignment was quick for you, I strongly recommend thinking about implementing softmax regression (logistic regression extended to the multi-class setting).

1. Implement one of your suggestions from your README so that you can properly compare these two algorithms. One concrete suggestion: either create a method that will convert ARFF to CSV, or visa versa. You will have to transform some of the features (and possibly outputs) along the way, but that way you won’t have to modify your algorithms directly.
<br><br>
For making logistic regression work in the multi-class setting, you have two options. You can use the likelihood function presented in class and run SGD. Or you can create $$K$$ separate binary classifiers that separate the data into class $$k$$ and all the other classes. Then choose the prediction with the highest probability.
<br><br>
2. So far we have not included regularization. Add an optional parameter `lambda` and explore adding regularization to the cost function and SGD. Did this improve the results? Did it improve the runtime?

 

## FAQ

**How do I know if my code is working?**  
Run the provided unit tests provided in [`tests.py`]().
Initially, many of them will fail. 
If your program is working correctly, all of them will pass. However, the converse is not true: passing all the tests is not sufficient to demonstrate that your code is working. 
*Using `tests.py` is strongly encouraged, this is similar to how your code will be graded on gradescope.*

## Submitting

Submit the following files to the assignment called `HW05` on Gradescope:

1. `run_LR.py`
2. `LogisticRegression.py`
3. Your readme

Make sure to name these files exactly what we specify here. Otherwise,
our autograders might not work and we might have to take points off.
Add any other files that you created that are needed for your code to run.

## Acknowledgements
Modified from lab by: Sara Mathieson, Ameet Soni, Jessica Wu.

