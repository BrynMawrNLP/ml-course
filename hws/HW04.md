---
layout: default
title: Naive Bayes
type: Homework
number: 04
active_tab: homework
release_date: 2024-09-30
due_date: 2024-10-08 23:59:00EDT

---

<!-- Check whether the assignment is ready to release -->
{% capture today %}{{'now' | date: '%s'}}{% endcapture %}
{% capture release_date %}{{page.release_date | date: '%s'}}{% endcapture %}
{% if release_date > today %} 
<div class="alert alert-danger">
Warning: this assignment is out of date.  It may still need to be updated for this year's class.  Check with your instructor before you start working on this assignment.
</div>
{% endif %}
<!-- End of check whether the assignment is up to date -->


<!-- Check whether the assignment is up to date -->
{% capture this_year %}{{'now' | date: '%Y'}}{% endcapture %}
{% capture due_year %}{{page.due_date | date: '%Y'}}{% endcapture %}
{% if this_year != due_year %} 
<div class="alert alert-danger">
Warning: this assignment is out of date.  It may still need to be updated for this year's class.  Check with your instructor before you start working on this assignment.
</div>
{% endif %}
<!-- End of check whether the assignment is up to date -->



{% if page.materials %}
<div class="alert alert-info">
You can download the materials for this assignment here:
<ul>
{% for item in page.materials %}
<li><a href="{{item.url}}">{{ item.name }}</a></li>
{% endfor %}
</ul>


<i>Remember to make a copy of the notebook into your own Drive by choosing “Save a Copy in Drive” from Colab’s “File” menu.</i>

</div>
{% endif %}





{{page.type}} {{page.number}}: {{page.title}}
=============================================================

_Due: {{page.due_date}}_

## Overview

The goals of this week’s assignment are:

- Understand the assumptions behind Naive Bayes
- Implement the Naive Bayes algorithm
- Practice multi-class prediction in a real-world setting


This week we will be analyzing the “zoo” dataset:

- Zoo data from [Zoo Animal Classification](https://www.kaggle.com/uciml/zoo-animal-classification) by Kaggle. The class labels are

```
Label	Name
0	Mammal
1	Bird
2	Reptile
3	Fish
4	Amphibian
5	Bug
6	Invertebrate
```

This assignment is to be done individually. Discussing high-level ideas with others in the class is encouraged, but you should not be sharing code in any way.

### Provided Code

You should have the following files:

- `input` - directory containing train/test data sets. You may add additional data sets here.
- `run_NB.py` - your main program executable for Naive Bayes.
- `NaiveBayes.py` - file for the NaiveBayes class.
- `README.md` - for analysis questions and lab feedback.

### Usage and I/O

#### Usage

Your programs should take in the same command-line arguments as HW02 (feel free to reuse this code). For example, to run Naive Bayes on the zoo example:

```unix
python run_NB.py -r input/zoo_train.arff -e input/zoo_test.arff
```

#### Program Inputs
To simplify preprocessing, you may assume the following:

- You may assume that all features are discrete (binary or multi-class) and unordered. You do not need to handle continuous features.
- Unlike HW02, prediction tasks can be multi-class. Think about how to modify the ARFF parser from HW02 to accommodate this.
- You can and should reuse as much as you can from HW02 (command-line arguments, ARFF file reading, `Example` class, `Partition` class, etc).

#### Program Outputs
- Your program should print a confusion matrix and accuracy of the result (see examples below).

- The design of your solution is largely up to you. However, you are required to create a `NaiveBayes` class.

## Naive Bayes
You will implement Naive Bayes as discussed in class. Your model will be capable of making multi-class predictions.

### Model

Our Naive Bayes model for the class label $$k$$ is:

$$
p(y=k | x)\propto p(y=k)\prod_{j=1}^{p}p(x_{j}|y=k)
$$

For this assignment, all probabilities should be modeled using log probabilities to avoid underflow. Note that when taking the log of a formula, all multiplications become additions and division become subtractions. First compute the log of the Naive Bayes model to make sure this makes sense.

### Training

You will need to represent model parameters ($$\theta$$’s) for each class probability $$p(y = k)$$ and each class conditional probability $$p(x_{j} = v|y = k)$$. 
This will require you to count the frequency of each event in your training data set. Use LaPlace estimates to avoid $$0$$ counts. If you think about the formula carefully, you will note that we can implement our training phase using only integers to keep track of counts (and no division!). Each theta is just a normalized frequency where we count how many examples where $$x_{j} = v$$ and the label is $$k$$ and divide by the total number of examples where the label is $$k$$ (plus the LaPlace estimate):

$$
\theta_{j,v,k}=\dfrac{N_{j,v,k}+1}{N_{k}+|f_{j}|}
$$

And similarly for the class probability estimates:

$$
\theta_{k} = \dfrac{N_{k}+1}{n+K}
$$

If we take the log then each theta becomes the difference between the two counts. The important point is that it is completely possible to train your Naive Bayes with only integers (the counts, N). You don’t need to calculate log scores until prediction time. This will make debugging (and training) much simpler.

### Inference/Prediction
For this phase, simply calculate the probability of each class (using the model equations above) and choose the maximum probability:

$$
\hat{y} = argmax_{k\in \{1,2\ldots,K\}} p(y=k)\prod_{j=1}^{p}p(x_{j}|y=k).
$$

### Suggested steps

1. Copy over your util.py file from HW02 for editing. You can get rid of everything related to continuous features. You also need to change the label from $$\{-1, 1\}$$ to $$\{0,1,2,\ldots,K-1\}$$. Think about how to do this in the `read_arff` function. You can also copy over the `Partition` and `Example` classes and put them in `util.py`. I would recommend adding an argument `K` to the `Partition` constructor so you can keep track of the number of classes.

2. Test these modifications in `run_NB.py` by creating a similar main to HW02. Something like this:
``` python
def main():

    opts = util.parse_args()
    train_partition = util.read_arff(opts.train_filename)
    test_partition  = util.read_arff(opts.test_filename)

    # sanity check
    print("num train =", train_partition.n, ", num classes =", train_partition.K)
    print("num test  =", test_partition.n, ", num classes =", test_partition.K)
```
Here is what the output should be for the “zoo” dataset:
```
num train = 66 , num classes = 7
num test  = 35 , num classes = 7
```

3. Create a `NaiveBayes` class in `NaiveBayes.py` and do all the work of training (creating probability structures) in the constructor. Then in main, I should be able to do something like:
``` python
nb_model = NaiveBayes(train_partition)
```
It is okay to have your constructor call other methods from within the class.


4. Write a `classify` method within the `NaiveBayes` class and pass in each test example:
``` python
y_hat = nb_model.classify(example.features)
```
And compute the accuracy by comparing these predictions to the ground truth.

### Example Output


- Tennis example:
```
$ python run_NB.py -r input/tennis_train.arff -e input/tennis_test.arff
Accuracy: 0.928571 (13 out of 14 correct)

  prediction
     0  1
   ------
 0|  4  1
 1|  0  9
```

- Zoo example:
```
$ python3 run_NB.py -r input/zoo_train.arff -e input/zoo_test.arff
Accuracy: 0.857143 (30 out of 35 correct)

        prediction
     0  1  2  3  4  5  6
   ---------------------
 0| 13  0  0  1  0  0  0
 1|  0  7  0  0  0  0  0
 2|  0  1  0  0  1  0  0
 3|  0  0  0  4  0  0  0
 4|  0  0  0  0  1  0  0
 5|  0  0  0  0  0  3  0
 6|  0  0  0  0  0  2  2
```

## Extension (optional)
In these examples, our features were discrete. Devise a way to make Naive Bayes work with continuous features. Discuss your idea in the README and/or implement it (zip code is an example dataset to test this on).

 
<!--
## FAQ

**How do I know if my code is working?**  
Run the provided unit tests provided in [`tests.py`]().
Initially, many of them will fail. 
If your program is working correctly, all of them will pass. However, the converse is not true: passing all the tests is not sufficient to demonstrate that your code is working. 
*Using `tests.py` is strongly encouraged, this is similar to how your code will be graded on gradescope.*
-->

## README.md

In a text file called `README.md` answer the following questions:

1. How much time did you spend on the homework
1. What did you learn from this homework
1. optional: What did you struggle with during this homework
1. optional: any other feedback you would like to share

## Submitting

Submit the following files to the assignment called `HW04` on Gradescope:

1. `NaiveBayes.py`
2. `run_NB.py`
3. Any other files necessary for your code to run.
3. Your readme

Make sure to name these files exactly what we specify here. Otherwise,
our autograders might not work and we might have to take points off.
Add any other files that you created that are needed for your code to run.

## Acknowledgements
Modified from: Sara Mathieson, Ameet Soni.

