---
layout: default
title: Polynomial Regression
type: Homework
number: 03
active_tab: homework
release_date: 2024-09-17
due_date: 2024-09-24 23:59:00EDT

---

<!-- Check whether the assignment is ready to release -->
{% capture today %}{{'now' | date: '%s'}}{% endcapture %}
{% capture release_date %}{{page.release_date | date: '%s'}}{% endcapture %}
{% if release_date > today %} 
<div class="alert alert-danger">
Warning: this assignment is out of date.  It may still need to be updated for this year's class.  Check with your instructor before you start working on this assignment.
</div>
{% endif %}
<!-- End of check whether the assignment is up to date -->


<!-- Check whether the assignment is up to date -->
{% capture this_year %}{{'now' | date: '%Y'}}{% endcapture %}
{% capture due_year %}{{page.due_date | date: '%Y'}}{% endcapture %}
{% if this_year != due_year %} 
<div class="alert alert-danger">
Warning: this assignment is out of date.  It may still need to be updated for this year's class.  Check with your instructor before you start working on this assignment.
</div>
{% endif %}
<!-- End of check whether the assignment is up to date -->



{% if page.materials %}
<div class="alert alert-info">
You can download the materials for this assignment here:
<ul>
{% for item in page.materials %}
<li><a href="{{item.url}}">{{ item.name }}</a></li>
{% endfor %}
</ul>


<i>Remember to make a copy of the notebook into your own Drive by choosing “Save a Copy in Drive” from Colab’s “File” menu.</i>

</div>
{% endif %}





{{page.type}} {{page.number}}: {{page.title}}
=============================================================

_Due: {{page.due_date}}_

## Overview

The goals of this week’s assignment are:

- To become more familiar with vector and matrix operations in code
- To investigate two different approaches to fitting regression models
- To think about the pros and cons of more flexible models

In this exercise, you will work through linear and polynomial regression. Our data consists of inputs $$x_{i} \in \mathbb{R}$$ and outputs $$y_{i} \in \mathbb{R}$$, which are related through a target function $$y = f(x) + \epsilon$$. Your goal is to learn a linear predictor $$h_{w}(x)$$ that best approximates $$f(x)$$.

This assignment is to be done individually. Discussing high-level ideas with others in the class is encouraged, but you should not be sharing code in any way.

### Provided Code

- code: `PolynomialRegression.py`, `run_regression.py`
- data : `regression_train.csv`, `regression_test.csv` (in the data folder)
- writeup: `writeup.tex` or `README.md` (choose one format for your writeup)

### Numpy Background
We used <i><b>numpy</b></i> before in Homework01, but we’ll use more of its functionality in this homework. It is a good skill to pick up since you will inevitably use $$\texttt{numpy}$$ if you plan to do math in Python, e.g. for machine learning. You may find it useful to work through a numpy tutorial first.[^1]

[^1]: Try out Numpy’s tutorial ([https://numpy.org/doc/stable/user/quickstart.html](https://numpy.org/doc/stable/user/quickstart.html)). Those familiar with Matlab may find the “Numpy for Matlab Users” documentation ([https://numpy.org/doc/stable/user/numpy-for-matlab-users.html](https://numpy.org/doc/stable/user/numpy-for-matlab-users.html)) more helpful.

Here are some things to keep in mind as you complete this problem:

- If you are seeing many errors at runtime, inspect your matrix operations to make sure that you are adding and multiplying matrices of compatible dimensions. Printing the dimensions of variables with the $$\texttt{X.shape}$$ command will help you debug.

- When working with $$\texttt{numpy}$$ arrays, note that $$\texttt{numpy}$$ interprets the $$\texttt{*}$$ operator as element-wise multiplication. This is a common source of size incompatibility errors. If you want matrix multiplication, you need to use the $$\texttt{np.dot}$$ function. For example, $$\texttt{A*B}$$ does element-wise multiplication while $$\texttt{np.dot(A,B)}$$ does a matrix multiply.

- Be careful when handling $$\texttt{numpy}$$ vectors (rank-1 arrays): the vector shapes $$1 × n$$, $$n × 1$$, and $$n$$ are all different things. For simplicity in this assignment (unless otherwise indicated in the code), both column and row vectors are rank-1 arrays of shape $$n$$, not rank-2 arrays of shape $$n × 1$$ or shape $$1×n$$.

Useful numpy functions:

- $$\texttt{np.array([...])}$$: takes in a list (or list-of-lists, etc) and turns it into a $$\texttt{numpy}$$ array. Note that after this step, you cannot $$\texttt{append}$$, etc – you have to use $$\texttt{numpy}$$ operations to create new array objects.

- $$\texttt{np.dot(A,B)}$$, $$\texttt{np.matmul(A,B)}$$: perform matrix multiplication. Inner dimensions of $$A$$ and $$B$$ should match.

- $$\texttt{np.ones((p,q))}$$: creates a 2D array of 1’s with the given shape (here it would be $$p×q$$). Similarly, you can use $$\texttt{np.zeros((p,q))}$$ to create an array of 0’s. If you include only one dimension (i.e. $$\texttt{np.zeros(p)}$$), this will yield a vector.

- $$\texttt{np.concatenate((A, B), axis=0)}$$: concatenate two arrays along the given axis. Here it would be along the rows (axis 0), so $$A$$ would be “on top”, and $$B$$ below. If we concatenate along axis 1 then $$A$$ would be on the “left” and $$B$$ would be on the right.

- $$\texttt{np.reshape(A, (p,q))}$$: will reshape array $$A$$ to have dimensions $$p × q$$ (or whatever dimensions you need, provided the values of $$A$$ can actually be reshaped in this way).

- $$\texttt{np.linalg.pinv(A)}$$: returns the pseudo-inverse of matrix $$A$$ (we will use this just in case we have issues with a singular matrix).

## 1. Visualization

Visualizations can help us understand the data.
For this data set, you can use a scatter plot since the data has only two properties to plot ($$x$$ and $$y$$).

- (a) Visualize the training and test data using the `plot_data(...)` function. What do you observe? For example, can you make an educated guess on the effectiveness of linear and polynomial regression in predicting the data? Include your responses and plots of both training and test data in your writeup.

<i>Hint</i>: As you implement the remaining exercises, use this plotting function as a debugging tool.

## 2. Linear Regression
Recall that the objective of linear regression is to minimize the cost function:

$$J(w) = \dfrac{1}{2} \sum_{i=1}^{n} (h_{w}(x_{i}) - y)^{2} $$

In this problem, we will use the matrix-vector form where

$$
 y = 
\begin{pmatrix}
y_{1} \\ y_{2} \\ \ldots \\ y_{n}
\end{pmatrix},
\hspace{25pt}
X = 
\begin{pmatrix}
 \\ --x_{1}-- \\ --x_{2}--\\ \ldots \\ --x_{n}--
\end{pmatrix},
\hspace{25pt}
 w = 
\begin{pmatrix}
w_{1} \\ w_{2} \\ \ldots \\ w_{p}
\end{pmatrix}
$$

where each example $$x_{i} = [1, x_{i1}, \ldots , x_{ip}]$$. Our linear regression model is: $$h_{w}(x) = w^{T}x$$.

Beginning with the simple linear regression case ($$p = 1$$), we have: $$h_{w}(x) = w_{0} + w_{1}x$$

- (b) 
Modify `generate_polynomial_features(...)` in `PolynomialRegression` to create the matrix $$X$$ for a simple linear model.
<br>
Note that to take into account the intercept term ($$w_{0}$$), we can add an additional ''feature'' to each example and set it to one, e.g. $$x_{i0} = 1$$. This is equivalent to adding an additional first column to $$X$$ and setting it to all ones.

- (c) Before tackling the harder problem of training the regression model, complete `predict(...)` in `PolynomialRegression` to predict $$y$$ from $$X$$ and $$w$$.

- (d) One way to solve linear regression is through stochastic gradient descent (SGD).
<br>
Recall that the parameters of our model are the $$w_{j}$$ values. These are the values we will adjust to minimize cost $$J(w)$$. In SGD, each iteration runs through the training set and performs the update
<br>
<br>
$$
\hspace{50pt}w_{j} 	\leftarrow w_{j}  - \alpha (h_{w}(x_{i}) - y_{i}) x_{ij} \text{  (simultaneously update wj for all j).}
$$ 
<br>
<br>
With each step of gradient descent, our parameters $$w_{j}$$ come closer to the optimal values that will
achieve the lowest cost $$J(w)$$.
<br>
	- As we perform gradient descent, it is helpful to monitor the convergence by computing the
cost. Complete `cost(...)` in `PolynomialRegression` to calculate $$J(w)$$.
<br>
<br>
	- Next, implement the gradient descent step in `fit_SGD(...)` in `PolynomialRegression`. The loop structure has been written for you, so you only need to supply the updates to w and the new predictions $$\hat{y}$$ within each iteration. In your writeup, include a plot of the final fit to the training data for $$\alpha = 0.01$$.
	<br> <i>Hint</i>: A good way to verify that gradient descent is working correctly is to look at the value of $$J(w)$$ and check that it is decreasing with each step. If you set `verbose=True` when calling `fit_SGD(...)`, then, assuming you have implemented gradient descent and `cost(...)` correctly, your value of `J(w)` should never increase and should converge to a steady value by the end of the algorithm.
	<br> <i>Hint</i>: With `verbose=True`, you may also find it useful to look at the 2D plots of the training data and the output of the trained regression model to verify that the model looks reasonable and is improving on each step.
<br>
<br>
	- So far, we have used a default learning rate (or step size) of $$\alpha = 0.01$$. Try different $$\alpha = 10^{-4},10^{-3},10^{-2},10^{-1}, and make a table of the coefficients and number of iterations until convergence (in your writeup). How do the coeffcients compare? How quickly does each algorithm converge?
<br>
<br>
- (e) In class, we learned that the closed-form solution to linear regression is
$$w = (X^{T} X)^{-1}X^{T}y.$$ Using this formula, you will get an exact solution in one calculation: there is no “loop until convergence” like in gradient descent.

	- Implement the closed-form solution `fit(...)` in `PolynomialRegression`.
	- writeup: How do the coeffcients compare to those obtained by SGD? How does your comparison change as $$\alpha$$ changes?
	- Use Python’s `time` module to measure the run time of the two approaches. You can capture the current time with `time.time()` (differences between these values are measured in seconds). How do the run times compare? writeup

## 2. Polynomial Regression


## FAQ

**How do I know if my code is working?**  
Run the provided unit tests provided in [`tests.py`]().
Initially, many of them will fail. 
If your program is working correctly, all of them will pass. However, the converse is not true: passing all the tests is not sufficient to demonstrate that your code is working. 
*Using `tests.py` is strongly encouraged, this is similar to how your code will be graded on gradescope.*

## README.md

In a text file called `README.md` answer the following questions:

1. How much time did you spend on the homework
1. What did you learn from this homework
1. optional: What did you struggle with during this homework
1. optional: any other feedback you would like to share

## Submitting

Submit the following files to the assignment called `HW02` on Gradescope:

README.md 
1. `run_dtree.py`
2. `Parition.py`
3. `DecisionTree.py`
2. `README.md` 

Make sure to name these files exactly what we specify here. Otherwise,
our autograders might not work and we might have to take points off.
Add any other files that you created that are needed for your code to run.

## Acknowledgements
Modified from lab by: Sara Mathieson, Ameet Soni.

Both data sets were made available by the UC Irvine Machine Learning Repository. The heart disease data set is the result of a study by the Cleveland Clinic to study coronary heart disease. The patients were identified as having or not having heart disease and the resulting feature set is a subset of the original 76 factors that were studied. The diabetes data set is the result of a study of females at least 21 years old of Pima Indian heritage to understand risk factors of diabetes. Both data sets were converted to ARFF format by Mark Craven at the University of Wisconsin.
